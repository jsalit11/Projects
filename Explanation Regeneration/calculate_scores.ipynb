{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "calculate_scores.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "2MZ-pOA9sxBX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3328b2f3-67cd-46e1-c15b-fe274e34d417"
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import logging\n",
        "import json\n",
        "import re\n",
        "import string\n",
        "import nltk\n",
        "import random\n",
        "from itertools import combinations\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.models import KeyedVectors\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logging = logging.getLogger(__name__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EZjvjZbvt7IG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed3de7d0-a93b-4519-9f92-fcf930487c84"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "base_dir = 'drive/My Drive/knowledge engineering/assignments/assignment_4/'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8oNTX6OWt8An"
      },
      "source": [
        "class ProcessData:\n",
        "    def __init__(self, input_directory, output_directory):\n",
        "        self.input_directory = input_directory\n",
        "        self.output_directory = output_directory\n",
        "\n",
        "    def load_table(self, file_path):        \n",
        "        with open(file_path) as f:\n",
        "            table = f.readlines()   \n",
        "        return table    \n",
        "\n",
        "    def get_tables(self):\n",
        "        file_list = os.listdir(self.input_directory)\n",
        "        return file_list \n",
        "\n",
        "    def iterate_tables(self):\n",
        "        processed_tables = []\n",
        "        for file_name in self.get_tables():\n",
        "            if file_name.endswith('.tsv'):\n",
        "                file_path = os.path.join(self.input_directory, file_name)\n",
        "                table = self.load_table(file_path)\n",
        "                processed_table = self.process_text(table)\n",
        "                processed_tables.append(processed_table)\n",
        "        return processed_tables\n",
        "\n",
        "    def create_sentence(self, row):       \n",
        "        sentence = ' '.join(row)\n",
        "        sentence += '.'\n",
        "        return sentence\n",
        "    \n",
        "    def clean_row(self, row):\n",
        "        row = row.lower()\n",
        "        row = re.split('\\t| ', row)[:-1]\n",
        "        row = [i for i in row if len(i) != 0]\n",
        "        re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
        "        row = [re_punc.sub('', word) for word in row]        \n",
        "\n",
        "        row = [word for word in row if word.isalpha()]       \n",
        "        return row\n",
        "\n",
        "    def process_text(self, table):\n",
        "        processed_table = ''\n",
        "        for row in table[1:]:\n",
        "            processed_row = self.clean_row(row)\n",
        "            processed_row = self.create_sentence(processed_row)\n",
        "            processed_table += processed_row\n",
        "        return processed_table\n",
        "\n",
        "    def main(self):\n",
        "        processed_tables = self.iterate_tables()\n",
        "        return processed_tables"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g7UAWQhbAyvO"
      },
      "source": [
        "class CalculateScores:\n",
        "    def __init__(self, processed_data, output_directory, pretrained=None):\n",
        "        self.processed_data = processed_data\n",
        "        self.output_directory = output_directory\n",
        "        self.pretrained_embeddings = self.get_pretrained_embeddings(pretrained)\n",
        "        self.pretrained_sample_terms = self.get_pretrained_sample_terms()\n",
        "        self.pretrained_scores = {}\n",
        "\n",
        "    def get_tfidf_scores(self):\n",
        "        tfidf = TfidfVectorizer(ngram_range = (1,1))\n",
        "        tfidf_matrix = tfidf.fit_transform(self.processed_data)\n",
        "        matrix = pd.DataFrame(tfidf_matrix.toarray(), columns = tfidf.get_feature_names())\n",
        "        tfidf_dict = matrix.max().to_dict()\n",
        "        return tfidf_dict\n",
        "\n",
        "    def prepare_word2vec(self):\n",
        "        word2vec_data = []\n",
        "        for table in self.processed_data:\n",
        "            table = table.split('.')\n",
        "            for sentence in table:\n",
        "                word2vec_data.append(sentence.split(' '))\n",
        "        return word2vec_data\n",
        "\n",
        "    def calculate_word2vec(self):\n",
        "        word2vec_data = self.prepare_word2vec()\n",
        "        word_vectors = Word2Vec(word2vec_data, size=100, min_count=2, window=10)\n",
        "        return word_vectors\n",
        "\n",
        "    def upload_models(self, tfidf, word2vec):\n",
        "        logging.info(f'Uploading models to {self.output_directory}')\n",
        "        word2vec.wv.save(os.path.join(self.output_directory, 'word2vec.wordvectors'))\n",
        "        with open(os.path.join(self.output_directory, 'tfidf.json'), 'w') as f:\n",
        "            json.dump(tfidf, f)\n",
        "\n",
        "    def get_pretrained_embeddings(self, pretrained):\n",
        "        if pretrained is None:\n",
        "            file_path = os.path.join(self.output_directory, \n",
        "                                    'GoogleNews-vectors-negative300.bin.gz')\n",
        "            pretrained = KeyedVectors.load_word2vec_format(file_path, binary = True)\n",
        "        return pretrained\n",
        "    \n",
        "    def get_pretrained_sample_terms(self):\n",
        "        sample_terms = random.sample(list(self.pretrained_embeddings.wv.vocab), 25)\n",
        "        return sample_terms\n",
        "\n",
        "    def get_pretrained_embedding_score(self, token):        \n",
        "        token_pretrained_score = 0\n",
        "        for idx, sample_term in enumerate(self.pretrained_sample_terms):\n",
        "            if token not in list(self.pretrained_embeddings.wv.vocab):\n",
        "                return 0\n",
        "            score = self.pretrained_embeddings.similarity(token, sample_term)\n",
        "            token_pretrained_score += score\n",
        "        token_pretrained_score /= len(self.pretrained_sample_terms)\n",
        "        self.pretrained_scores[token] = token_pretrained_score\n",
        "        return token_pretrained_score\n",
        "\n",
        "    def rerank_tfidf(self):\n",
        "        reranked_tfidf_scores = {}\n",
        "        tfidf_scores = self.get_tfidf_scores()\n",
        "        for idx, term in enumerate(list(tfidf_scores.keys())):\n",
        "            if idx / 100 == 0:\n",
        "                logging.info(f'reranked {idx} terms, on term {term}')\n",
        "            score = tfidf_scores.get(term)\n",
        "            pretrained_score = self.get_pretrained_embedding_score(term)\n",
        "            score -= pretrained_score\n",
        "            reranked_tfidf_scores[term] = score\n",
        "        return reranked_tfidf_scores\n",
        "\n",
        "    def rerank_tfidf_2(self, tfidf_matrix, word2vec):    \n",
        "        embed_rerank = pd.DataFrame(0, columns = tfidf_matrix.columns, index=tfidf_matrix.index)    \n",
        "        pre_embed_rerank = pd.DataFrame(0, columns = tfidf_matrix.columns, index=tfidf_matrix.index)    \n",
        "        for doc_num in range(len(tfidf_matrix)):\n",
        "            logging.info(f'Calculating rerank tfidf scores for document {doc_num}')\n",
        "            filtered_terms = tfidf_matrix.iloc[doc_num][tfidf_matrix.iloc[doc_num] > 0]\n",
        "            terms_combinations = combinations(filtered_terms.index, 1)\n",
        "            filtered_terms = filtered_terms.sample(50)\n",
        "            logging.info(f'Number of terms in document {doc_num} is {len(filtered_terms.index)}')\n",
        "            sample_terms = random.sample(list(tfidf_matrix.iloc[doc_num][tfidf_matrix.iloc[doc_num] > 0].index), 20)\n",
        "            for idx, term in enumerate(filtered_terms.index):\n",
        "                logging.info(f'Term {term}')\n",
        "                if idx / 100 == 0:\n",
        "                    logging.info(f'Calculated scores for {idx} terms')\n",
        "                for sample_term in sample_terms:\n",
        "                    if (term not in list(self.pretrained_embeddings.wv.vocab)) or (sample_term not in list(self.pretrained_embeddings.wv.vocab)):\n",
        "                        continue                \n",
        "                    elif (term not in list(word2vec.wv.vocab)) or (sample_term not in list(word2vec.wv.vocab)):\n",
        "                        continue\n",
        "                    embed_rerank[term].iloc[doc_num] += word2vec.similarity(term, sample_term)\n",
        "                    pre_embed_rerank[term].iloc[doc_num] += self.pretrained_embeddings.similarity(term, sample_term)\n",
        "            embed_rerank.iloc[doc_num] /= len(sample_terms)\n",
        "            pre_embed_rerank.iloc[doc_num] /= len(sample_terms)\n",
        "            if doc_num == 2:\n",
        "                break\n",
        "        return embed_rerank, pre_embed_rerank\n",
        "\n",
        "    def main(self):\n",
        "        tfidf = self.rerank_tfidf()        \n",
        "        word2vec = self.calculate_word2vec()\n",
        "        self.upload_models(tfidf, word2vec)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vV2ovC-3Jcek"
      },
      "source": [
        "def main(input_directory, output_directory, pretrained):\n",
        "    processed_tables = ProcessData(input_directory, \n",
        "                                output_directory\n",
        "                                ).main()    \n",
        "        calculate_scores = CalculateScores(processed_tables, output_directory, pretrained)\n",
        "    return calculate_scores"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-rRJZcxzKt4r"
      },
      "source": [
        "input_directory = os.path.join(base_dir, 'data/worldtree_full/tsv/tables')\n",
        "output_directory = os.path.join(base_dir, 'data/mapping_files')\n",
        "obj = main(input_directory, output_directory, pretrained).main()"
      ],
      "execution_count": 1,
      "outputs": []
    }
  ]
}