{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Experiment_6.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "xt5HSqgHFvF0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "import h5py\n",
        "import os\n",
        "import time\n",
        "from random import sample\n",
        "import datetime\n",
        "from collections import Counter\n",
        "import sys\n",
        "from tqdm import tqdm, notebook\n",
        "from tensorflow.keras.datasets import reuters\n",
        "pd.options.display.max_colwidth = 100\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.layers import Conv2D\n",
        "from tensorflow.keras.layers import MaxPooling2D\n",
        "from tensorflow.keras.layers import Activation\n",
        "from tensorflow.keras.layers import Flatten, Embedding\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.losses import sparse_categorical_crossentropy\n",
        "from keras.utils import plot_model\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras import preprocessing\n",
        "from tensorflow.keras.layers import Embedding, SimpleRNN, LSTM, GRU, Conv1D, MaxPooling1D\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from sklearn.model_selection import ParameterGrid\n",
        "\n",
        "word_index = reuters.get_word_index()\n",
        "tf.keras.backend.set_floatx('float64')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7xHipHzjF1vY",
        "colab_type": "code",
        "outputId": "7104071c-0702-4fd6-c9cd-b43992edb28b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount = True)\n",
        "root_dir = '/content/gdrive/My Drive/'\n",
        "base_dir = root_dir + 'Northwestern/Artificial Intelligence and Deep Learning/Assignment 3/'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GlfND62eF1yu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def import_data(voc_size):\n",
        "\n",
        "    (train_data_raw, train_labels_raw), (test_data_raw, test_labels_raw) = reuters.load_data(num_words=voc_size)\n",
        "    # word_index = reuters.get_word_index()\n",
        "\n",
        "    return train_data_raw, train_labels_raw, test_data_raw, test_labels_raw"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r_r5InuIF119",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def top_label_data(X, y):\n",
        "\n",
        "    topics_tpl, topics_freq = zip(*Counter(y).most_common(9))\n",
        "    X_top, y_top = zip(*((x_samp, y_samp) for x_samp, y_samp in zip(X, y) if y_samp in topics_tpl))\n",
        "    X_top, y_top = np.array(X_top), np.array(y_top)\n",
        "\n",
        "    keys = Counter(y_top).keys()\n",
        "    conv = dict(zip(sorted(keys), range(0,9)))\n",
        "    y_top = [conv[l] for l in y_top]\n",
        "\n",
        "    return X_top, y_top"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZLgMSchbF15V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def max_len(train_data, train_labels, percentile, top):\n",
        "\n",
        "    if top == False:\n",
        "        pass\n",
        "\n",
        "    elif top == True:\n",
        "        train_data, train_labels = top_label_data(train_data, train_labels)\n",
        "\n",
        "    train_data_len = [len(w) for w in train_data]\n",
        "    maxlen = int(np.percentile(train_data_len, percentile))\n",
        "\n",
        "    return maxlen"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s4sUyy5a_aOD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def vectorize_sequences(sequences, dimension = 10000):\n",
        "\n",
        "    results = np.zeros((len(sequences), dimension))\n",
        "    for i, sequence, in enumerate(sequences):\n",
        "        results[i, sequence] = 1.\n",
        "\n",
        "    return results"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tsAn8f4NF181",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def return_max_lens(train_data, train_labels, top):\n",
        "\n",
        "    max_lengths = []\n",
        "    for percentile in np.arange(20, 120, 20):\n",
        "        ml = max_len(train_data, train_labels, percentile, top = top)\n",
        "        max_lengths.append(ml)\n",
        "\n",
        "    return max_lengths"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6awAqPkzF2AV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def prepare_data(train_data, train_labels, test_data, test_labels, max_len,\n",
        "                 top, embedding = False):\n",
        "\n",
        "    if top == False:\n",
        "        pass\n",
        "\n",
        "    elif top == True:\n",
        "        train_data, train_labels = top_label_data(train_data, train_labels)\n",
        "        test_data, test_labels = top_label_data(test_data, test_labels)\n",
        "\n",
        "    if embedding == False:\n",
        "        train_data = vectorize_sequences(train_data)\n",
        "        test_data = vectorize_sequences(test_data)\n",
        "        # maxlen = None\n",
        "        \n",
        "    elif embedding == True:\n",
        "        # maxlen = max_len(train_data, train_labels, top)\n",
        "        train_data = preprocessing.sequence.pad_sequences(train_data, maxlen=max_len)\n",
        "        test_data = preprocessing.sequence.pad_sequences(test_data, maxlen=max_len)\n",
        "\n",
        "    train_data, val_data, train_labels, val_labels = train_test_split(\n",
        "        train_data, train_labels, test_size = 0.15)\n",
        "    train_labels = to_categorical(train_labels)\n",
        "    val_labels = to_categorical(val_labels)\n",
        "    test_labels = to_categorical(test_labels)\n",
        "\n",
        "    return train_data, train_labels, val_data, val_labels, test_data, test_labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "adDyd1xMF2Ds",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tensor_slices(train_data, train_labels, val_data, val_labels, test_data, test_labels):\n",
        "\n",
        "    train_ds = tf.data.Dataset.from_tensor_slices((train_data, train_labels)).shuffle(100).batch(100)\n",
        "    val_ds = tf.data.Dataset.from_tensor_slices((val_data, val_labels)).batch(100)\n",
        "    test_ds = tf.data.Dataset.from_tensor_slices((test_data, test_labels))\n",
        "\n",
        "    return train_ds, val_ds, test_ds"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eQnjFFhsF2HN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def embedding_matrix(embedding_file, voc_size):\n",
        "\n",
        "    embeddings_index = {}\n",
        "    f = open(os.path.join(base_dir, embedding_file))\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        coefs = np.asarray(values[1:], dtype = 'float32')\n",
        "        embeddings_index[word] = coefs\n",
        "    f.close()\n",
        "\n",
        "    embedding_dim = int(embedding_file.split('.')[-2].replace('d', ''))\n",
        "\n",
        "    embedding_matrix = np.zeros((voc_size, embedding_dim))\n",
        "    for word, i in word_index.items():\n",
        "        if i < voc_size:\n",
        "            embedding_vector = embeddings_index.get(word)\n",
        "            if embedding_vector is not None:\n",
        "                embedding_matrix[i] = embedding_vector\n",
        "\n",
        "    return embedding_matrix, embedding_dim"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l4NJdtEaF2N1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def model_train(features, labels, model):\n",
        "    \n",
        "    with tf.GradientTape() as tape:\n",
        "        predictions = model(features)\n",
        "        loss = loss_func(labels, predictions)\n",
        "\n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "\n",
        "    train_loss.update_state(loss)\n",
        "    train_acc.update_state(labels, predictions)\n",
        "\n",
        "    return gradients"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "51kSdt3EF2RE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def model_validate(features, labels, model):\n",
        "    \n",
        "    predictions = model(features)\n",
        "    v_loss = loss_func(labels, predictions)\n",
        "\n",
        "    valid_loss.update_state(v_loss)\n",
        "    valid_acc.update_state(labels, predictions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cLJ7gHibF2UN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_loss = tf.keras.metrics.Mean(name=\"train_loss\")\n",
        "valid_loss = tf.keras.metrics.Mean(name=\"test_loss\")\n",
        "test_loss = tf.keras.metrics.CategoricalCrossentropy(name=\"test_loss\")\n",
        "\n",
        "train_acc = tf.keras.metrics.CategoricalAccuracy(name=\"train_acc\")\n",
        "valid_acc = tf.keras.metrics.CategoricalAccuracy(name=\"valid_acc\")\n",
        "test_acc = tf.keras.metrics.CategoricalAccuracy(name=\"test_acc\")\n",
        "\n",
        "loss_func = tf.keras.losses.CategoricalCrossentropy()\n",
        "optimizer = tf.keras.optimizers.RMSprop()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MM9O0K6HGiKr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def df_performance(performance, model_desc):\n",
        "\n",
        "    df_performance = pd.DataFrame(performance).iloc[-1:]\n",
        "    df_performance.insert(0, 'model', model_desc)\n",
        "    file_path = os.path.join(base_dir, 'best_model_table.csv')\n",
        "\n",
        "    if os.path.exists(file_path):\n",
        "        df_performance_tot = pd.read_csv(file_path)\n",
        "        df_performance = pd.concat([df_performance_tot, df_performance])\n",
        "    else:\n",
        "        pass\n",
        "\n",
        "    df_performance.to_csv(file_path, index = False)\n",
        "\n",
        "    return df_performance.round(3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PEDeenEFGO3O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def model_run(model, max_len, top = False, embedding = False, pretrained_embed = False):\n",
        "    \n",
        "    keras.backend.clear_session()\n",
        "\n",
        "    train_data_raw, train_labels_raw, test_data_raw, test_labels_raw = import_data(10000)\n",
        "    train_data, train_labels, val_data, val_labels, test_data, test_labels = prepare_data(\n",
        "        train_data_raw, train_labels_raw, test_data_raw, test_labels_raw, max_len, top = top, embedding = embedding)\n",
        "    train_data, val_data, test_slice = tensor_slices(train_data, train_labels, \n",
        "                                                    val_data, val_labels, \n",
        "                                                    test_data, test_labels)\n",
        "    \n",
        "\n",
        "    EPOCHS = 20\n",
        "    train_acc_history = []\n",
        "    train_loss_history = []\n",
        "    val_acc_history = []\n",
        "    val_loss_history = []\n",
        "    weight_history = []\n",
        "    elapsed_time_tot = []\n",
        "    gradients = []\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "\n",
        "        start_time = time.time()\n",
        "        for features, labels in train_data:\n",
        "            grad = model_train(features, labels, model)\n",
        "        gradients.append(grad)\n",
        "        \n",
        "        for val_features, val_labels in val_data:\n",
        "            model_validate(val_features, val_labels, model)\n",
        "\n",
        "        loss, acc = train_loss.result(), train_acc.result()\n",
        "        val_loss, val_acc = valid_loss.result(), valid_acc.result()\n",
        "\n",
        "        train_acc_history.append(acc.numpy())\n",
        "        train_loss_history.append(loss.numpy())\n",
        "        val_acc_history.append(val_acc.numpy())\n",
        "        val_loss_history.append(val_loss.numpy())\n",
        "        weight_history.append([layer.get_weights() for layer in model.layers])\n",
        "\n",
        "        train_loss.reset_states(), train_acc.reset_states()\n",
        "        valid_loss.reset_states(), valid_acc.reset_states()\n",
        "\n",
        "        end_time = time.time()\n",
        "        elapsed_time = end_time - start_time\n",
        "        elapsed_time_tot.append(elapsed_time)\n",
        "        \n",
        "        template = \"\"\"Epoch {}, loss: {:.3f}, acc: {:.3f}, val_loss: {:.3f}, val_acc: {:.3f}, elapsed time: {:.3f}\"\"\"\n",
        "        print (template.format(epoch+1,\n",
        "                            loss,\n",
        "                            acc,\n",
        "                            val_loss,\n",
        "                            val_acc,\n",
        "                            elapsed_time))\n",
        "        \n",
        "    elapsed_time_tot = [sum(elapsed_time_tot[0:x:1]) for x in range(1, EPOCHS+1)]\n",
        "        \n",
        "    predictions = model(test_data, training = False)\n",
        "    t_acc = test_acc(test_labels, predictions)\n",
        "    t_loss = loss_func(test_labels, predictions)   \n",
        "    print(f'Test accuracy: {t_acc:.3f}, Test loss: {t_loss:.3f}')    \n",
        "               \n",
        "        \n",
        "    performance = {\n",
        "        'train_acc': train_acc_history,\n",
        "        'train_loss': train_loss_history,\n",
        "        'val_acc': val_acc_history,\n",
        "        'val_loss': val_loss_history,\n",
        "        'test_acc': t_acc.numpy(),\n",
        "        'test_loss': t_loss.numpy(),   \n",
        "        'elapsed_time': elapsed_time_tot\n",
        "    }\n",
        "        \n",
        "    return model, weight_history, performance, gradients"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8OLSclcoIAA9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_model(top, pretrained_embed = False):\n",
        "\n",
        "    if top == False:\n",
        "        topics = 46\n",
        "    \n",
        "    elif top == True:\n",
        "        topics = 9\n",
        "\n",
        "    model = Sequential()\n",
        "\n",
        "    if pretrained_embed == True:\n",
        "        embedding_file = 'glove.6B.100d.txt'\n",
        "        embed_matrix, embed_dim = embedding_matrix(embedding_file)\n",
        "        model.layers[0].set_weights([embed_matrix])\n",
        "        model.layers[0].trainable = False\n",
        "        output_dim = embed_dim\n",
        "    elif pretrained_embed == False:\n",
        "        output_dim = 128\n",
        "\n",
        "    model.add(Embedding(input_dim = 10000, output_dim = output_dim, input_length = 280))\n",
        "    # model.add(Conv1D(filters= 16, kernel_size = 5, activation = 'relu'))\n",
        "    # model.add(MaxPooling1D(3))\n",
        "    # model.add(BatchNormalization())\n",
        "    # model.add(Dropout(0.25))\n",
        "    # model.add(Dense(16, activation = 'relu', kernel_regularizer='l2'))\n",
        "    # model.add(layers.GRU(units = 32, activation = 'tanh'))\n",
        "    # model.add(BatchNormalization())\n",
        "    # model.add(Dropout(0.5))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(32, activation = 'relu', kernel_regularizer='l2'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(64, activation = 'relu', kernel_regularizer='l2'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(64, activation = 'relu', kernel_regularizer='l2'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(0.5))\n",
        "    # model.add(Dense(32, activation = 'relu', kernel_regularizer='l2'))\n",
        "    # model.add(BatchNormalization())\n",
        "    # model.add(Dropout(0.8))\n",
        "    # model.add(Flatten())\n",
        "    # model.add(Dense(128, activation = 'relu', kernel_regularizer='l2'))\n",
        "    # model.add(BatchNormalization())\n",
        "    # model.add(Dropout(0.5))\n",
        "    model.add(Dense(units = topics, activation = 'softmax'))\n",
        "\n",
        "    model.compile(\n",
        "                    optimizer = 'rmsprop',\n",
        "                    loss = 'categorical_crossentropy',\n",
        "                    metrics = ['accuracy'])\n",
        "\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wdIr2PTlGO6I",
        "colab_type": "code",
        "outputId": "e35a0a7b-0574-4e14-9902-635ce133a92b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "top = False\n",
        "embedding = True\n",
        "pretrained_embed = True\n",
        "model, weight_history, performance, grads = model_run(build_model(top), 280, top = top, \n",
        "    embedding = embedding, pretrained_embed = pretrained_embed)\n",
        "df_performance(performance, 'dnn_1 pretrained no TOP')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1, loss: 2.134, acc: 0.439, val_loss: 1.797, val_acc: 0.567, elapsed time: 6.420\n",
            "Epoch 2, loss: 1.524, acc: 0.605, val_loss: 1.571, val_acc: 0.623, elapsed time: 6.361\n",
            "Epoch 3, loss: 1.109, acc: 0.717, val_loss: 1.514, val_acc: 0.640, elapsed time: 6.422\n",
            "Epoch 4, loss: 0.739, acc: 0.827, val_loss: 1.458, val_acc: 0.682, elapsed time: 6.508\n",
            "Epoch 5, loss: 0.478, acc: 0.891, val_loss: 1.555, val_acc: 0.665, elapsed time: 6.401\n",
            "Epoch 6, loss: 0.327, acc: 0.930, val_loss: 1.639, val_acc: 0.684, elapsed time: 6.335\n",
            "Epoch 7, loss: 0.243, acc: 0.943, val_loss: 1.820, val_acc: 0.673, elapsed time: 6.345\n",
            "Epoch 8, loss: 0.198, acc: 0.949, val_loss: 2.426, val_acc: 0.588, elapsed time: 6.310\n",
            "Epoch 9, loss: 0.171, acc: 0.951, val_loss: 2.089, val_acc: 0.655, elapsed time: 6.194\n",
            "Epoch 10, loss: 0.149, acc: 0.952, val_loss: 2.482, val_acc: 0.654, elapsed time: 6.272\n",
            "Epoch 11, loss: 0.139, acc: 0.952, val_loss: 2.345, val_acc: 0.657, elapsed time: 6.290\n",
            "Epoch 12, loss: 0.129, acc: 0.952, val_loss: 2.724, val_acc: 0.609, elapsed time: 6.431\n",
            "Epoch 13, loss: 0.118, acc: 0.954, val_loss: 2.505, val_acc: 0.659, elapsed time: 6.556\n",
            "Epoch 14, loss: 0.110, acc: 0.956, val_loss: 2.698, val_acc: 0.641, elapsed time: 6.412\n",
            "Epoch 15, loss: 0.105, acc: 0.956, val_loss: 2.794, val_acc: 0.616, elapsed time: 6.442\n",
            "Epoch 16, loss: 0.102, acc: 0.956, val_loss: 2.688, val_acc: 0.626, elapsed time: 6.481\n",
            "Epoch 17, loss: 0.095, acc: 0.956, val_loss: 2.758, val_acc: 0.645, elapsed time: 6.578\n",
            "Epoch 18, loss: 0.093, acc: 0.958, val_loss: 3.326, val_acc: 0.573, elapsed time: 6.469\n",
            "Epoch 19, loss: 0.090, acc: 0.959, val_loss: 3.292, val_acc: 0.568, elapsed time: 6.306\n",
            "Epoch 20, loss: 0.086, acc: 0.960, val_loss: 2.814, val_acc: 0.636, elapsed time: 6.332\n",
            "Test accuracy: 0.728, Test loss: 2.901\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>model</th>\n",
              "      <th>train_acc</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>val_acc</th>\n",
              "      <th>val_loss</th>\n",
              "      <th>test_acc</th>\n",
              "      <th>test_loss</th>\n",
              "      <th>elapsed_time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>cnn_test_1</td>\n",
              "      <td>0.991</td>\n",
              "      <td>0.037</td>\n",
              "      <td>0.769</td>\n",
              "      <td>2.099</td>\n",
              "      <td>0.750</td>\n",
              "      <td>2.353</td>\n",
              "      <td>233.583</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>cnn_test_2</td>\n",
              "      <td>0.982</td>\n",
              "      <td>0.041</td>\n",
              "      <td>0.769</td>\n",
              "      <td>1.479</td>\n",
              "      <td>0.756</td>\n",
              "      <td>1.651</td>\n",
              "      <td>90.137</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>cnn w/ GRU</td>\n",
              "      <td>0.961</td>\n",
              "      <td>0.138</td>\n",
              "      <td>0.721</td>\n",
              "      <td>1.183</td>\n",
              "      <td>0.743</td>\n",
              "      <td>1.257</td>\n",
              "      <td>100.981</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>dnn_1</td>\n",
              "      <td>0.973</td>\n",
              "      <td>0.049</td>\n",
              "      <td>0.748</td>\n",
              "      <td>1.448</td>\n",
              "      <td>0.747</td>\n",
              "      <td>1.508</td>\n",
              "      <td>89.051</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>dnn_2</td>\n",
              "      <td>0.968</td>\n",
              "      <td>0.054</td>\n",
              "      <td>0.763</td>\n",
              "      <td>1.427</td>\n",
              "      <td>0.750</td>\n",
              "      <td>1.543</td>\n",
              "      <td>86.556</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>dnn_3</td>\n",
              "      <td>0.974</td>\n",
              "      <td>0.047</td>\n",
              "      <td>0.768</td>\n",
              "      <td>1.240</td>\n",
              "      <td>0.753</td>\n",
              "      <td>1.311</td>\n",
              "      <td>83.492</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>cnn_3</td>\n",
              "      <td>0.964</td>\n",
              "      <td>0.065</td>\n",
              "      <td>0.801</td>\n",
              "      <td>1.294</td>\n",
              "      <td>0.757</td>\n",
              "      <td>1.358</td>\n",
              "      <td>82.754</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>cnn_4</td>\n",
              "      <td>0.963</td>\n",
              "      <td>0.090</td>\n",
              "      <td>0.802</td>\n",
              "      <td>1.108</td>\n",
              "      <td>0.762</td>\n",
              "      <td>1.203</td>\n",
              "      <td>84.652</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>cnn_5</td>\n",
              "      <td>0.963</td>\n",
              "      <td>0.096</td>\n",
              "      <td>0.800</td>\n",
              "      <td>1.034</td>\n",
              "      <td>0.767</td>\n",
              "      <td>1.136</td>\n",
              "      <td>54.403</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>cnn_lstm_nn_1</td>\n",
              "      <td>0.964</td>\n",
              "      <td>0.141</td>\n",
              "      <td>0.780</td>\n",
              "      <td>0.968</td>\n",
              "      <td>0.785</td>\n",
              "      <td>0.980</td>\n",
              "      <td>95.094</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>cnn_lstm_nn_2</td>\n",
              "      <td>0.962</td>\n",
              "      <td>0.141</td>\n",
              "      <td>0.805</td>\n",
              "      <td>0.980</td>\n",
              "      <td>0.787</td>\n",
              "      <td>1.097</td>\n",
              "      <td>88.820</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>cnn_lstm_nn_4</td>\n",
              "      <td>0.967</td>\n",
              "      <td>0.090</td>\n",
              "      <td>0.784</td>\n",
              "      <td>1.221</td>\n",
              "      <td>0.783</td>\n",
              "      <td>1.294</td>\n",
              "      <td>252.536</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>lstm_nn_1</td>\n",
              "      <td>0.947</td>\n",
              "      <td>0.195</td>\n",
              "      <td>0.751</td>\n",
              "      <td>1.143</td>\n",
              "      <td>0.770</td>\n",
              "      <td>1.244</td>\n",
              "      <td>111.850</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>lstm_nn_1 add l2</td>\n",
              "      <td>0.946</td>\n",
              "      <td>0.208</td>\n",
              "      <td>0.780</td>\n",
              "      <td>1.016</td>\n",
              "      <td>0.767</td>\n",
              "      <td>1.128</td>\n",
              "      <td>112.038</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>gru_nn_1 add l2</td>\n",
              "      <td>0.945</td>\n",
              "      <td>0.237</td>\n",
              "      <td>0.723</td>\n",
              "      <td>1.296</td>\n",
              "      <td>0.757</td>\n",
              "      <td>1.422</td>\n",
              "      <td>116.583</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>dnn_4 no embed</td>\n",
              "      <td>0.965</td>\n",
              "      <td>0.079</td>\n",
              "      <td>0.853</td>\n",
              "      <td>0.954</td>\n",
              "      <td>0.770</td>\n",
              "      <td>1.047</td>\n",
              "      <td>55.779</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>dnn_4 no embed high drop</td>\n",
              "      <td>0.964</td>\n",
              "      <td>0.085</td>\n",
              "      <td>0.837</td>\n",
              "      <td>0.949</td>\n",
              "      <td>0.779</td>\n",
              "      <td>0.988</td>\n",
              "      <td>55.203</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>dnn_4 no embed high drop</td>\n",
              "      <td>0.963</td>\n",
              "      <td>0.081</td>\n",
              "      <td>0.845</td>\n",
              "      <td>0.935</td>\n",
              "      <td>0.785</td>\n",
              "      <td>0.989</td>\n",
              "      <td>45.510</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>dnn_1 no embed high drop no TOP</td>\n",
              "      <td>0.954</td>\n",
              "      <td>0.118</td>\n",
              "      <td>0.760</td>\n",
              "      <td>1.489</td>\n",
              "      <td>0.783</td>\n",
              "      <td>1.639</td>\n",
              "      <td>55.604</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>cnn_nn_1 no TOP</td>\n",
              "      <td>0.955</td>\n",
              "      <td>0.137</td>\n",
              "      <td>0.652</td>\n",
              "      <td>2.173</td>\n",
              "      <td>0.770</td>\n",
              "      <td>2.217</td>\n",
              "      <td>119.193</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>cnn_lstm_nn_1 no TOP</td>\n",
              "      <td>0.951</td>\n",
              "      <td>0.167</td>\n",
              "      <td>0.665</td>\n",
              "      <td>2.219</td>\n",
              "      <td>0.759</td>\n",
              "      <td>2.292</td>\n",
              "      <td>150.897</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>cnn_gru_nn_1 pretrained no TOP</td>\n",
              "      <td>0.884</td>\n",
              "      <td>0.440</td>\n",
              "      <td>0.593</td>\n",
              "      <td>2.303</td>\n",
              "      <td>0.743</td>\n",
              "      <td>2.404</td>\n",
              "      <td>145.851</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>cnn_dnn_1 pretrained no TOP</td>\n",
              "      <td>0.954</td>\n",
              "      <td>0.177</td>\n",
              "      <td>0.660</td>\n",
              "      <td>2.086</td>\n",
              "      <td>0.735</td>\n",
              "      <td>2.320</td>\n",
              "      <td>132.440</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>dnn_1 pretrained no TOP</td>\n",
              "      <td>0.960</td>\n",
              "      <td>0.086</td>\n",
              "      <td>0.636</td>\n",
              "      <td>2.814</td>\n",
              "      <td>0.728</td>\n",
              "      <td>2.901</td>\n",
              "      <td>127.867</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                              model  train_acc  ...  test_loss  elapsed_time\n",
              "0                        cnn_test_1      0.991  ...      2.353       233.583\n",
              "1                        cnn_test_2      0.982  ...      1.651        90.137\n",
              "2                        cnn w/ GRU      0.961  ...      1.257       100.981\n",
              "3                             dnn_1      0.973  ...      1.508        89.051\n",
              "4                             dnn_2      0.968  ...      1.543        86.556\n",
              "5                             dnn_3      0.974  ...      1.311        83.492\n",
              "6                             cnn_3      0.964  ...      1.358        82.754\n",
              "7                             cnn_4      0.963  ...      1.203        84.652\n",
              "8                             cnn_5      0.963  ...      1.136        54.403\n",
              "9                     cnn_lstm_nn_1      0.964  ...      0.980        95.094\n",
              "10                    cnn_lstm_nn_2      0.962  ...      1.097        88.820\n",
              "11                    cnn_lstm_nn_4      0.967  ...      1.294       252.536\n",
              "12                        lstm_nn_1      0.947  ...      1.244       111.850\n",
              "13                 lstm_nn_1 add l2      0.946  ...      1.128       112.038\n",
              "14                  gru_nn_1 add l2      0.945  ...      1.422       116.583\n",
              "15                   dnn_4 no embed      0.965  ...      1.047        55.779\n",
              "16         dnn_4 no embed high drop      0.964  ...      0.988        55.203\n",
              "17         dnn_4 no embed high drop      0.963  ...      0.989        45.510\n",
              "18  dnn_1 no embed high drop no TOP      0.954  ...      1.639        55.604\n",
              "19                  cnn_nn_1 no TOP      0.955  ...      2.217       119.193\n",
              "20             cnn_lstm_nn_1 no TOP      0.951  ...      2.292       150.897\n",
              "21   cnn_gru_nn_1 pretrained no TOP      0.884  ...      2.404       145.851\n",
              "22      cnn_dnn_1 pretrained no TOP      0.954  ...      2.320       132.440\n",
              "19          dnn_1 pretrained no TOP      0.960  ...      2.901       127.867\n",
              "\n",
              "[24 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 133
        }
      ]
    }
  ]
}