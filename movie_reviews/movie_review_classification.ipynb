{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment 3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "wSi8H8_8xBRF",
        "outputId": "913dcad1-2507-4cf1-a7cc-8218d50a013c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import os\n",
        "import re,string\n",
        "import nltk\n",
        "import re\n",
        "import spacy\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "import bs4\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "from tqdm import tqdm\n",
        "from collections import defaultdict\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.manifold import MDS\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "from sklearn.metrics import silhouette_score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import roc_auc_score, accuracy_score, confusion_matrix\n",
        "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "\n",
        "from gensim.models import Word2Vec,LdaMulticore, TfidfModel\n",
        "from gensim import corpora\n",
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "from gensim.models import KeyedVectors\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM, Flatten, Dropout\n",
        "\n",
        "!pip install nlpia\n",
        "from nlpia.data.loaders import get_data\n",
        "!pip install python-docx\n",
        "from docx import Document\n",
        "import time\n",
        "from itertools import combinations\n",
        "from itertools import permutations\n",
        "from itertools import chain\n",
        "from collections import Counter\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.layers import Conv2D\n",
        "from tensorflow.keras.layers import MaxPooling2D\n",
        "from tensorflow.keras.layers import Activation\n",
        "from tensorflow.keras.layers import Flatten, Embedding\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.losses import sparse_categorical_crossentropy\n",
        "from keras.utils import plot_model\n",
        "from tensorflow.keras import preprocessing\n",
        "from tensorflow.keras.layers import Embedding, SimpleRNN\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.models import Model\n",
        "from keras.utils import to_categorical\n",
        "from sklearn.preprocessing import LabelEncoder"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "Collecting nlpia\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/89/f6/ab35e962dd0b19f1008e88e788b202d45a90d9cd70b9bbf0ac26489ee260/nlpia-0.5.2-py2.py3-none-any.whl (32.0MB)\n",
            "\u001b[K     |████████████████████████████████| 32.0MB 134kB/s \n",
            "\u001b[?25hRequirement already satisfied: jupyter in /usr/local/lib/python3.6/dist-packages (from nlpia) (1.0.0)\n",
            "Collecting pypandoc\n",
            "  Downloading https://files.pythonhosted.org/packages/d6/b7/5050dc1769c8a93d3ec7c4bd55be161991c94b8b235f88bf7c764449e708/pypandoc-1.5.tar.gz\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from nlpia) (0.22.2.post1)\n",
            "Requirement already satisfied: pandas-datareader in /usr/local/lib/python3.6/dist-packages (from nlpia) (0.8.1)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.6/dist-packages (from nlpia) (4.2.6)\n",
            "Collecting python-Levenshtein\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/42/a9/d1785c85ebf9b7dfacd08938dd028209c34a0ea3b1bcdb895208bd40a67d/python-Levenshtein-0.12.0.tar.gz (48kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 5.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorflow in /usr/local/lib/python3.6/dist-packages (from nlpia) (2.3.0)\n",
            "Requirement already satisfied: html5lib in /usr/local/lib/python3.6/dist-packages (from nlpia) (1.0.1)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.6/dist-packages (from nlpia) (2.4.3)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from nlpia) (2.10.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from nlpia) (4.41.1)\n",
            "Collecting html2text\n",
            "  Downloading https://files.pythonhosted.org/packages/ae/88/14655f727f66b3e3199f4467bafcc88283e6c31b562686bf606264e09181/html2text-2020.1.16-py3-none-any.whl\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.6/dist-packages (from nlpia) (0.10.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from nlpia) (3.2.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from nlpia) (3.2.5)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.6/dist-packages (from nlpia) (4.4.1)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.6/dist-packages (from nlpia) (2.2.4)\n",
            "Collecting pugnlp\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f7/20/a9f0b1f45c074c63da716fc1b301916cc3b64c11d5cbf7cb305eafaf158a/pugnlp-0.2.6-py2.py3-none-any.whl (706kB)\n",
            "\u001b[K     |████████████████████████████████| 716kB 50.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from nlpia) (1.0.5)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from nlpia) (2019.12.20)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from nlpia) (0.16.0)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.6/dist-packages (from nlpia) (3.6.0)\n",
            "Requirement already satisfied: qtconsole in /usr/local/lib/python3.6/dist-packages (from jupyter->nlpia) (4.7.6)\n",
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.6/dist-packages (from jupyter->nlpia) (7.5.1)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.6/dist-packages (from jupyter->nlpia) (5.6.1)\n",
            "Requirement already satisfied: jupyter-console in /usr/local/lib/python3.6/dist-packages (from jupyter->nlpia) (5.2.0)\n",
            "Requirement already satisfied: notebook in /usr/local/lib/python3.6/dist-packages (from jupyter->nlpia) (5.3.1)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.6/dist-packages (from jupyter->nlpia) (4.10.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from pypandoc->nlpia) (49.6.0)\n",
            "Requirement already satisfied: pip>=8.1.0 in /usr/local/lib/python3.6/dist-packages (from pypandoc->nlpia) (19.3.1)\n",
            "Requirement already satisfied: wheel>=0.25.0 in /usr/local/lib/python3.6/dist-packages (from pypandoc->nlpia) (0.35.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->nlpia) (0.16.0)\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->nlpia) (1.18.5)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->nlpia) (1.4.1)\n",
            "Requirement already satisfied: requests>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from pandas-datareader->nlpia) (2.23.0)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow->nlpia) (0.3.3)\n",
            "Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow->nlpia) (1.1.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow->nlpia) (3.3.0)\n",
            "Requirement already satisfied: tensorboard<3,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->nlpia) (2.3.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow->nlpia) (1.12.1)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->nlpia) (1.15.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->nlpia) (0.8.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow->nlpia) (0.2.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.4.0,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->nlpia) (2.3.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow->nlpia) (1.31.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow->nlpia) (3.12.4)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->nlpia) (1.1.0)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow->nlpia) (1.6.3)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.6/dist-packages (from html5lib->nlpia) (0.5.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras->nlpia) (3.13)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->nlpia) (2.8.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->nlpia) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->nlpia) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->nlpia) (1.2.0)\n",
            "Requirement already satisfied: retrying>=1.3.3 in /usr/local/lib/python3.6/dist-packages (from plotly->nlpia) (1.3.3)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy->nlpia) (2.0.3)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy->nlpia) (1.0.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy->nlpia) (1.0.2)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy->nlpia) (7.4.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy->nlpia) (3.0.2)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy->nlpia) (0.7.1)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy->nlpia) (1.1.3)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy->nlpia) (0.4.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy->nlpia) (1.0.2)\n",
            "Requirement already satisfied: coverage in /usr/local/lib/python3.6/dist-packages (from pugnlp->nlpia) (3.7.1)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.6/dist-packages (from pugnlp->nlpia) (4.0.1)\n",
            "Collecting fuzzywuzzy\n",
            "  Downloading https://files.pythonhosted.org/packages/43/ff/74f23998ad2f93b945c0309f825be92e04e0348e062026998b5eefef4c33/fuzzywuzzy-0.18.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->nlpia) (2018.9)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from gensim->nlpia) (2.1.0)\n",
            "Requirement already satisfied: jupyter-client>=4.1 in /usr/local/lib/python3.6/dist-packages (from qtconsole->jupyter->nlpia) (5.3.5)\n",
            "Requirement already satisfied: qtpy in /usr/local/lib/python3.6/dist-packages (from qtconsole->jupyter->nlpia) (1.9.0)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from qtconsole->jupyter->nlpia) (0.2.0)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from qtconsole->jupyter->nlpia) (2.1.3)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.6/dist-packages (from qtconsole->jupyter->nlpia) (4.6.3)\n",
            "Requirement already satisfied: pyzmq>=17.1 in /usr/local/lib/python3.6/dist-packages (from qtconsole->jupyter->nlpia) (19.0.2)\n",
            "Requirement already satisfied: traitlets in /usr/local/lib/python3.6/dist-packages (from qtconsole->jupyter->nlpia) (4.3.3)\n",
            "Requirement already satisfied: nbformat>=4.2.0 in /usr/local/lib/python3.6/dist-packages (from ipywidgets->jupyter->nlpia) (5.0.7)\n",
            "Requirement already satisfied: ipython>=4.0.0; python_version >= \"3.3\" in /usr/local/lib/python3.6/dist-packages (from ipywidgets->jupyter->nlpia) (5.5.0)\n",
            "Requirement already satisfied: widgetsnbextension~=3.5.0 in /usr/local/lib/python3.6/dist-packages (from ipywidgets->jupyter->nlpia) (3.5.1)\n",
            "Requirement already satisfied: jinja2>=2.4 in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->nlpia) (2.11.2)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->nlpia) (0.6.0)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->nlpia) (3.1.5)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->nlpia) (1.4.2)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->nlpia) (0.3)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->nlpia) (0.8.4)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->nlpia) (0.4.4)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from jupyter-console->jupyter->nlpia) (1.0.18)\n",
            "Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from notebook->jupyter->nlpia) (0.8.3)\n",
            "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.6/dist-packages (from notebook->jupyter->nlpia) (1.5.0)\n",
            "Requirement already satisfied: tornado>=4 in /usr/local/lib/python3.6/dist-packages (from notebook->jupyter->nlpia) (5.1.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.3.0->pandas-datareader->nlpia) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.3.0->pandas-datareader->nlpia) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.3.0->pandas-datareader->nlpia) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.3.0->pandas-datareader->nlpia) (2020.6.20)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow->nlpia) (3.2.2)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow->nlpia) (1.7.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow->nlpia) (1.17.2)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow->nlpia) (1.0.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow->nlpia) (0.4.1)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy->nlpia) (1.7.0)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.6/dist-packages (from python-slugify->pugnlp->nlpia) (1.3)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim->nlpia) (1.14.47)\n",
            "Requirement already satisfied: boto in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim->nlpia) (2.49.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from traitlets->qtconsole->jupyter->nlpia) (4.4.2)\n",
            "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.6/dist-packages (from nbformat>=4.2.0->ipywidgets->jupyter->nlpia) (2.6.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets->jupyter->nlpia) (0.7.5)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets->jupyter->nlpia) (4.8.0)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets->jupyter->nlpia) (0.8.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2>=2.4->nbconvert->jupyter->nlpia) (1.1.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from bleach->nbconvert->jupyter->nlpia) (20.4)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.0.0,>=1.0.0->jupyter-console->jupyter->nlpia) (0.2.5)\n",
            "Requirement already satisfied: ptyprocess; os_name != \"nt\" in /usr/local/lib/python3.6/dist-packages (from terminado>=0.8.1->notebook->jupyter->nlpia) (0.6.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow->nlpia) (4.6)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow->nlpia) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow->nlpia) (4.1.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow->nlpia) (1.3.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy->nlpia) (3.1.0)\n",
            "Requirement already satisfied: botocore<1.18.0,>=1.17.47 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim->nlpia) (1.17.47)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim->nlpia) (0.3.3)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim->nlpia) (0.10.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3\"->google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow->nlpia) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow->nlpia) (3.1.0)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.18.0,>=1.17.47->boto3->smart-open>=1.2.1->gensim->nlpia) (0.15.2)\n",
            "Building wheels for collected packages: pypandoc, python-Levenshtein\n",
            "  Building wheel for pypandoc (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pypandoc: filename=pypandoc-1.5-cp36-none-any.whl size=17038 sha256=7294d8209282786b6c9dde3c9a646040dc220440fec368120b2dde23f6e55a6a\n",
            "  Stored in directory: /root/.cache/pip/wheels/bb/7d/d6/2f9af55e800d37e42e546106bcbd36a86e24e725e303d17e04\n",
            "  Building wheel for python-Levenshtein (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-Levenshtein: filename=python_Levenshtein-0.12.0-cp36-cp36m-linux_x86_64.whl size=144793 sha256=5b96b5f09a3b5eed84b7902a7aa0438b6efaf0b7b31f2f4f643381c73ccfcd28\n",
            "  Stored in directory: /root/.cache/pip/wheels/de/c2/93/660fd5f7559049268ad2dc6d81c4e39e9e36518766eaf7e342\n",
            "Successfully built pypandoc python-Levenshtein\n",
            "Installing collected packages: pypandoc, python-Levenshtein, html2text, fuzzywuzzy, pugnlp, nlpia\n",
            "Successfully installed fuzzywuzzy-0.18.0 html2text-2020.1.16 nlpia-0.5.2 pugnlp-0.2.6 pypandoc-1.5 python-Levenshtein-0.12.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/pugnlp/constants.py:158: FutureWarning: The pandas.datetime class is deprecated and will be removed from pandas in a future version. Import from datetime instead.\n",
            "  MIN_TIMESTAMP = pd.Timestamp(pd.datetime(1677, 9, 22, 0, 12, 44), tz='utc')\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Collecting python-docx\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e4/83/c66a1934ed5ed8ab1dbb9931f1779079f8bca0f6bbc5793c06c4b5e7d671/python-docx-0.8.10.tar.gz (5.5MB)\n",
            "\u001b[K     |████████████████████████████████| 5.5MB 7.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: lxml>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from python-docx) (4.2.6)\n",
            "Building wheels for collected packages: python-docx\n",
            "  Building wheel for python-docx (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-docx: filename=python_docx-0.8.10-cp36-none-any.whl size=184491 sha256=d02265f1b2074b1bfecba7f2c51169b2dba47870965879cbb588f98881a5af4e\n",
            "  Stored in directory: /root/.cache/pip/wheels/18/0b/a0/1dd62ff812c857c9e487f27d80d53d2b40531bec1acecfa47b\n",
            "Successfully built python-docx\n",
            "Installing collected packages: python-docx\n",
            "Successfully installed python-docx-0.8.10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8ZOYzoTxDIg",
        "outputId": "fda778aa-4360-4938-86bd-485a1aa1a634",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount = True)\n",
        "root_dir = '/content/gdrive/My Drive/'\n",
        "base_dir = root_dir + 'Natural Language Processing/assignments/assignment_3/'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WpcXVElwxIIu"
      },
      "source": [
        "def load_corpus(file_name):\n",
        "    df = pd.read_csv(base_dir + file_name, index_col = 0)\n",
        "    titles = df['DSI_Title'].values\n",
        "    text = df['Text'].values\n",
        "    return titles, text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y4yvw2W7xMj9"
      },
      "source": [
        "def clean_doc(doc, word2vec = False):\n",
        "    re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    if word2vec == False:\n",
        "        tokens = doc.split()\n",
        "        tokens = lower_first_word(tokens)\n",
        "        tokens = [re_punc.sub('', w) for w in tokens]\n",
        "        tokens = [word for word in tokens if word.isalpha()]\n",
        "        tokens = [word for word in tokens if len(word) > 5]\n",
        "        # # tokens = [word.lower() for word in tokens]\n",
        "        tokens = [w for w in tokens if not w in stop_words]\n",
        "        # tokens = [PorterStemmer().stem(word) for word in tokens]\n",
        "    elif word2vec == True:\n",
        "        tokens = sent_tokenize(doc)\n",
        "        tokens = [word.split() for word in tokens]\n",
        "        tokens = [[re_punc.sub('', word) for word in sent] for sent in tokens]\n",
        "        tokens = [[word for word in sent if word.isalpha()] for sent in tokens]\n",
        "        tokens = [[word for word in sent if len(word) > 4] for sent in tokens]\n",
        "        tokens = [[word.lower() for word in sent] for sent in tokens]\n",
        "        tokens = [[word for word in sent if not word in stop_words] for sent in tokens]\n",
        "        # tokens = [[PorterStemmer().stem(word) for word in sent] for sent in tokens]\n",
        "    return tokens    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "397IMSkKxNbt"
      },
      "source": [
        "def lower_first_word(tokens):\n",
        "    for idx, word in enumerate(tokens):\n",
        "        if '.' in word:\n",
        "            if idx + 1 != len(tokens):\n",
        "                tokens[idx + 1] = tokens[idx + 1].lower()\n",
        "        if idx == 0:\n",
        "            tokens[idx] = tokens[idx].lower()\n",
        "    return tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2JExtq9xOl2"
      },
      "source": [
        "def process_text(file_name, method):\n",
        "    titles, corpus = load_corpus(file_name)\n",
        "    processed_text = []\n",
        "    for text in corpus:\n",
        "        if method == 'tfidf':\n",
        "            text = clean_doc(text)\n",
        "            temp_text = ''\n",
        "            for word in text:\n",
        "                temp_text += word + ' '\n",
        "            processed_text.append(temp_text)\n",
        "        elif method == 'word2vec':\n",
        "            text = clean_doc(text, word2vec = True)\n",
        "            processed_text.extend(text)\n",
        "        elif method == 'doc2vec':\n",
        "            text = clean_doc(text)\n",
        "            processed_text.append(text)            \n",
        "        elif method == 'entity_extract':            \n",
        "            # processed_text.append(text.lower().split('.'))\n",
        "            text = clean_doc(text, word2vec = True)\n",
        "            temp_text = []\n",
        "            for sent in text:\n",
        "                temp_sent = ''\n",
        "                for word in sent:\n",
        "                    temp_sent += word + ' '\n",
        "                temp_text.append(temp_sent)\n",
        "            processed_text.append(temp_text)\n",
        "    return titles, processed_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AOrC7DPDxQsu"
      },
      "source": [
        "def df_tfidf(processed_text):\n",
        "    titles, text = processed_text\n",
        "    tfidf = TfidfVectorizer(ngram_range = (1,3))\n",
        "    tfidf_matrix = tfidf.fit_transform(text)\n",
        "    matrix = pd.DataFrame(tfidf_matrix.toarray(), columns = tfidf.get_feature_names(),\n",
        "                      index = titles)\n",
        "    return matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UHSSfHbjxQ6e"
      },
      "source": [
        "def get_top_tfidf_terms(df_tfidf, top_num):\n",
        "    top_term = {}\n",
        "    for idx in range(len(df_tfidf)):\n",
        "        row_max = max(df_tfidf.iloc[idx])\n",
        "        for word in df_tfidf.columns:\n",
        "            if df_tfidf[word].iloc[idx] == row_max:\n",
        "                top_term[df_tfidf.index[idx]] = word\n",
        "    return top_term"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c3tIe-0nxSfG"
      },
      "source": [
        "def get_top_tfidf_terms(df_tfidf, top_num, value_range=False):\n",
        "    top_term = {}\n",
        "    for idx in range(len(df_tfidf)):\n",
        "        large_values = df_tfidf.iloc[idx].nlargest(top_num)\n",
        "        if value_range == False:\n",
        "            term = large_values.index[top_num - 1]\n",
        "            value = large_values.values[top_num - 1]\n",
        "            top_term[df_tfidf.index[idx]] = term\n",
        "        elif value_range == True:\n",
        "            term = large_values.index[: (top_num - 1)]\n",
        "            value = large_values.values[: (top_num - 1)]\n",
        "            top_term[df_tfidf.index[idx]] = term\n",
        "    return top_term"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BBEd3ca4xMan"
      },
      "source": [
        "def parse_sentences(sent):\n",
        "    ent1 = \"\"\n",
        "    ent2 = \"\"\n",
        "    prv_tok_dep = \"\"\n",
        "    prv_tok_text = \"\"\n",
        "    prefix = \"\"\n",
        "    modifier= \"\"\n",
        "    token = \"\"\n",
        "\n",
        "    remove_words = set()\n",
        "    add_words = set()\n",
        "    remove_from_remove_words = set()\n",
        "    not_person_type = set()\n",
        "    for tok in nlp(sent):\n",
        "        if tok.dep_ == \"advmod\":\n",
        "            remove_words.add(tok.text)\n",
        "        if tok.dep_ == 'amod':\n",
        "            remove_words.add(tok.text)\n",
        "        if tok.ent_type_ == 'PERSON':\n",
        "            remove_words.add(tok.text)\n",
        "        else: \n",
        "            not_person_type.add(tok.text)\n",
        "    return remove_words, not_person_type"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YklMNofKxdiY"
      },
      "source": [
        "def get_remove_words():\n",
        "    docs = process_text('class_corpus.csv', 'entity_extract')\n",
        "    entities = set()\n",
        "    add_entities = set()\n",
        "    entities_dict = {}\n",
        "    for idx, doc in enumerate(docs[1]):\n",
        "        for sent in doc:\n",
        "            temp_rem_list, temp_tot_list = parse_sentences(sent)\n",
        "            for word in temp_rem_list:\n",
        "                entities.add(word)\n",
        "                if word in entities_dict.keys():\n",
        "                    entities_dict[word]['pers_count'] += 1\n",
        "                else:\n",
        "                    entities_dict[word] = {'pers_count': 1, 'tot_count': 0}\n",
        "            for word in temp_tot_list:\n",
        "                if word in entities_dict.keys():\n",
        "                    entities_dict[word]['tot_count'] += 1\n",
        "                else:\n",
        "                    entities_dict[word] = {'pers_count': 0, 'tot_count': 1}   \n",
        "    entities_copy = entities.copy()\n",
        "    for word in entities_copy:\n",
        "        if entities_dict[word]['pers_count'] < entities_dict[word]['tot_count'] / 2:\n",
        "            entities.remove(word)\n",
        "    return entities"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vVwWrn-44-9R"
      },
      "source": [
        "remove_words = get_remove_words()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lMROsetWxStm"
      },
      "source": [
        "class KnowledgeGraph:\n",
        "    def __init__(self):\n",
        "        self.graph = nx.DiGraph()\n",
        "\n",
        "    def get_top_terms(self, num_terms, method, all_doc_term_num=None, doc_term_num=None):\n",
        "        top_terms_dict = {}\n",
        "        if method == 'tfidf':        \n",
        "            top_terms = get_top_tfidf_terms(df_tfidf(process_text('class_corpus.csv', \n",
        "                                            'tfidf')), num_terms, value_range=True)\n",
        "            for key_review, values in top_terms.items():\n",
        "                nodes = combinations(values, 2)\n",
        "                top_terms_dict[key_review] = list(nodes)\n",
        "        elif method == 'raw':\n",
        "            top_terms_dict = get_top_raw_terms(all_doc_term_num, doc_term_num)\n",
        "        elif method == 'entity_extraction':\n",
        "            top_terms_dict = get_top_terms_entity_extraction()\n",
        "        return top_terms_dict\n",
        "\n",
        "    def filter_top_terms(self, term, num_terms, method):\n",
        "        filtered_terms = {}\n",
        "        top_terms = self.get_top_terms( num_terms, method, all_doc_term_num=None, doc_term_num=None)\n",
        "        for key_review, values in top_terms.items():\n",
        "            for x, y in values:\n",
        "                if term in x:\n",
        "                    if key_review in filtered_terms.keys():\n",
        "                        filtered_terms[key_review].append((x, y))\n",
        "                    else:\n",
        "                        filtered_terms[key_review] = [(x, y)]\n",
        "                elif term in y:\n",
        "                    if key_review in filtered_terms.keys():\n",
        "                        filtered_terms[key_review].append((x, y))\n",
        "                    else:\n",
        "                        filtered_terms[key_review] = [(x, y)]\n",
        "        return filtered_terms\n",
        "\n",
        "    def set_node_attributes(self):\n",
        "        edge_attributes = nx.get_edge_attributes(self.graph, 'review')\n",
        "        for keys, value in edge_attributes.items():\n",
        "            nx.set_node_attributes(self.graph, {keys[0]:value}, 'review')\n",
        "            nx.set_node_attributes(self.graph, {keys[1]:value}, 'review')\n",
        "\n",
        "    def get_node_class_dict(self):\n",
        "        iterable = nx.get_edge_attributes(self.graph, 'review')\n",
        "        node_class_dict = {}\n",
        "        for keys, value in iterable.items():\n",
        "            if keys[0] in node_class_dict.keys():\n",
        "                if value in node_class_dict[keys[0]]:\n",
        "                    pass\n",
        "                else:\n",
        "                    node_class_dict[keys[0]].append(value)\n",
        "            else:\n",
        "                node_class_dict[keys[0]] = [value]\n",
        "            if keys[1] in node_class_dict.keys():\n",
        "                if value in node_class_dict[keys[0]]:\n",
        "                    pass\n",
        "                else:\n",
        "                    node_class_dict[keys[1]].append(value)\n",
        "            else:\n",
        "                node_class_dict[keys[1]] = [value]\n",
        "        return node_class_dict\n",
        "\n",
        "    def process_kg(self, method, num_terms=None, term=None, filtered=False,\n",
        "                   all_doc_term_num=None, doc_term_num=None):\n",
        "        if filtered == False:\n",
        "            data = self.get_top_terms(num_terms, method, all_doc_term_num, doc_term_num)\n",
        "            for key_review, nodes in data.items():\n",
        "                self.graph.add_edges_from(nodes, review=key_review)\n",
        "        elif filtered == True:\n",
        "            data = self.filter_top_terms(term, num_terms, method)\n",
        "            for key_review, nodes in data.items():\n",
        "                self.graph.add_edges_from(nodes, review=key_review)\n",
        "        self.set_node_attributes()\n",
        "\n",
        "    def get_top_classes(self, num_degrees):\n",
        "        top_classes = []\n",
        "        for node, degree in self.graph.degree:\n",
        "            if degree > num_degrees:\n",
        "                top_classes.append(node)\n",
        "        top_class_dict = {}\n",
        "        node_class_dict = self.get_node_class_dict()\n",
        "        for node in top_classes:\n",
        "            top_class_dict[node] = node_class_dict.get(node)\n",
        "        return top_class_dict\n",
        "\n",
        "    def get_reviews_class(self, term):\n",
        "        node_class_dict = self.get_node_class_dict()\n",
        "        return node_class_dict.get(term)\n",
        "\n",
        "    def filter_movie(self, terms):\n",
        "        filtered_dict = {}\n",
        "        for term in terms:\n",
        "            for edges, value in nx.get_edge_attributes(self.graph, 'review').items():\n",
        "                if value == term:\n",
        "                    filtered_dict[edges] = value\n",
        "        kg_filter = nx.DiGraph()\n",
        "        kg_filter.add_edges_from(filtered_dict.keys(), review=filtered_dict.values())\n",
        "        self.graph_kg(graph_data=kg_filter)\n",
        "   \n",
        "    def graph_kg(self, graph_data=None, attributes=False):\n",
        "        if graph_data is None:\n",
        "            graph_data = self.graph\n",
        "        plt.figure(figsize = [12, 12])\n",
        "        pos = nx.spring_layout(graph_data, k=1)\n",
        "        nx.draw(graph_data, with_labels=True, node_color='skyblue', \n",
        "                edge_cmap=plt.cm.Blues, pos=pos, width=0.5,\n",
        "                font_size=20, arrows=False)\n",
        "        if attributes == True:\n",
        "            edge_labels = nx.get_edge_attributes(graph_data, 'review')\n",
        "            nx.draw_networkx_edge_labels(graph_data, pos, labels=edge_labels)\n",
        "\n",
        "    def graph_abstract_nodes(self, num_degrees, exclude_words=None, attributes=False):\n",
        "        graph = nx.DiGraph()\n",
        "        for node, degree in self.graph.degree:\n",
        "            if exclude_words is not None: \n",
        "                if node not in exclude_words:                \n",
        "                    if degree > num_degrees:\n",
        "                        graph.add_node(node)\n",
        "            else:\n",
        "                if degree > num_degrees:\n",
        "                    graph.add_node(node)\n",
        "        for edge in list(self.graph.edges):\n",
        "            if (edge[0] in graph.nodes) and (edge[1] in graph.nodes):\n",
        "                graph.add_edge(edge[0], edge[1])\n",
        "        self.graph_kg(graph_data=graph, attributes=attributes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5cZvkeMNKuHm"
      },
      "source": [
        "class BFSGraph:\n",
        "    def __init__(self, kg):\n",
        "        self.graph = kg\n",
        "        self.visited_dict = defaultdict(list)\n",
        "        self.queue = defaultdict(set)\n",
        "        self.class_movie_rev_map = defaultdict(set)\n",
        "        self.test_node_attributes = []\n",
        "\n",
        "    def BFS(self, node, edge=None):\n",
        "        self.queue[node].add(node)\n",
        "        if edge is None:\n",
        "            queue = list(self.graph.edges(node))\n",
        "        else:\n",
        "            queue = list(self.graph.edges(edge))\n",
        "        while queue:\n",
        "            s = queue.pop(0)\n",
        "            for _, i in self.graph.edges(s):    \n",
        "                time.sleep(0.0)            \n",
        "                if i not in self.queue[node]:\n",
        "                    self.test_node_attributes.append(nx.get_node_attributes(kg.graph, 'review').get(i))\n",
        "                    self.queue[node].add(i)\n",
        "                    temp_movie_reviews = nx.get_node_attributes(kg.graph, 'review').get(i)\n",
        "                    self.class_movie_rev_map[node].add(temp_movie_reviews)\n",
        "                    self.BFS(node, edge=i)\n",
        "\n",
        "    def run(self):\n",
        "        node_degrees = [len(self.graph.edges(node)) for node in self.graph.nodes]\n",
        "        node_deg_std = np.std(node_degrees)\n",
        "        node_deg_mean = np.mean(node_degrees)\n",
        "        for node in self.graph.nodes('circle'):\n",
        "            # if len(self.graph.edges(node)) > node_deg_mean + node_deg_std:\n",
        "            if len(self.graph.edges(node)) > node_deg_mean:\n",
        "                self.BFS(node[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s52HqI4DbYt3",
        "outputId": "bcf83c51-8f42-471e-cbc8-d858e9b8396b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "word_vectors = KeyedVectors.load_word2vec_format(base_dir + 'GoogleNews-vectors-negative300.bin.gz', binary = True)\n",
        "word_vectors_corpus = Word2Vec(process_text('class_corpus.csv', 'word2vec')[1], size=100, min_count=3, window=6)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:254: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2020-08-27 10:53:43,177 WARNING:gensim.models.base_any2vec:1386:      _log_train_end under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rrs3aY6PZQsL"
      },
      "source": [
        "class WordVectors:\n",
        "    def __init__(self, kg, bfsgraph, word_vectors, word_vectors_corpus, remove_words, num_terms):\n",
        "        self.kg = kg\n",
        "        self.bfsgraph = bfsgraph\n",
        "        self.word_vectors = word_vectors\n",
        "        self.word_vectors_corpus = word_vectors_corpus\n",
        "        self.filtered_queue = defaultdict(set)\n",
        "        self.remove_words = remove_words\n",
        "        self.num_terms = num_terms\n",
        "\n",
        "    def filter_list(self):\n",
        "        for key, values in self.bfsgraph.queue.items():\n",
        "            for value in values:\n",
        "                if ((value in self.word_vectors.wv.vocab) and \n",
        "                    (value in self.word_vectors_corpus.wv.vocab)\n",
        "                    ):\n",
        "                    self.filtered_queue[key].add(value)\n",
        "\n",
        "    def get_similarities(self):\n",
        "        self.similarity_dict = {}\n",
        "        self.similarity_dict_corpus = {}\n",
        "        self.similarity_dict_movie = {}\n",
        "        self.filter_list()\n",
        "        for key, values in self.filtered_queue.items():\n",
        "            self.similarity_dict[key] = defaultdict(list)\n",
        "            self.similarity_dict_corpus[key] = defaultdict(list)\n",
        "            self.similarity_dict_movie[key] = defaultdict(list)\n",
        "            for i in permutations(values, 2):\n",
        "                self.similarity_dict[key][i[0]].append(self.word_vectors.similarity(i[0], i[1]))\n",
        "                self.similarity_dict_corpus[key][i[0]].append(self.word_vectors_corpus.similarity(i[0], i[1]))\n",
        "                self.similarity_dict_movie[key][i[0]] = self.word_vectors_corpus.similarity(i[0], 'movie')\n",
        "\n",
        "    def get_similarity_scores(self, weights=None):\n",
        "        self.get_similarities()\n",
        "        similarity_dict_results = {}\n",
        "        for key, values in self.similarity_dict.items():\n",
        "            # similarity_dict_results[key] = defaultdict(dict)\n",
        "            similarity_dict_results[key] = {}\n",
        "            for key_term in values.keys():\n",
        "                wv_value = np.mean(self.similarity_dict[key][key_term])\n",
        "                wv_corpus_value = np.mean(self.similarity_dict_corpus[key][key_term])\n",
        "                wv_movie_value = self.similarity_dict_movie[key][key_term]\n",
        "                if weights is not None:\n",
        "                    similarity_dict_results[key][key_term] = weights[0] * wv_value + weights[1] * wv_corpus_value + weights[2] * wv_movie_value\n",
        "                else:\n",
        "                    weights = [0.5, 0.25, 0.25]\n",
        "                    similarity_dict_results[key][key_term] = weights[0] * wv_value + weights[1] * wv_corpus_value + weights[2] * wv_movie_value\n",
        "        return similarity_dict_results\n",
        "\n",
        "    def get_max_similarities(self, weights=None, max_weights=None):\n",
        "        similarity_scores = self.get_similarity_scores(weights)\n",
        "        max_similarity_scores = defaultdict(dict)\n",
        "        # max_similarity_scores = {}\n",
        "        for key, values in similarity_scores.items():\n",
        "            if len(values.values()) > 0:\n",
        "                sorted_scores = sorted(values.items(), key=lambda x: x[1], reverse=True)[:5]\n",
        "                for term in sorted_scores:\n",
        "                    num_nodes = len(bfsgraph.class_movie_rev_map[term[0]])\n",
        "                    kg_deg = len(self.kg.graph.edges(term[0]))\n",
        "                    max_similarity_scores[key][term[0]] = {'num_nodes': num_nodes,\n",
        "                                                            'sim_score': term[1],\n",
        "                                                            'kg_deg': kg_deg}\n",
        "        return max_similarity_scores     \n",
        "\n",
        "    def calculate_scores(self, weights=None, max_weights=None):\n",
        "        max_similarity_scores = self.get_max_similarities(weights, max_weights)\n",
        "        num_nodes_max = []\n",
        "        sim_score_max = []\n",
        "        kg_deg_max = []\n",
        "        for values in max_similarity_scores.values():\n",
        "            for terms, scores in values.items():\n",
        "                num_nodes_max.append(scores['num_nodes'])\n",
        "                sim_score_max.append(scores['sim_score'])\n",
        "                kg_deg_max.append(scores['kg_deg'])\n",
        "        num_nodes_max = max(num_nodes_max)\n",
        "        sim_score_max = max(sim_score_max)\n",
        "        kg_deg_max = max(kg_deg_max)\n",
        "        if max_weights is None:\n",
        "            max_weights = {}\n",
        "            max_weights['num_nodes'] = 0.25\n",
        "            max_weights['sim_score'] = 0.4\n",
        "            max_weights['kg_deg'] = 0.35\n",
        "        for key in max_similarity_scores.keys():\n",
        "            for max_val_key, max_val_val in max_similarity_scores[key].items():\n",
        "                if max_val_val['num_nodes'] > 0:\n",
        "                    num_nodes_score = (max_val_val['num_nodes'] / num_nodes_max) * max_weights['num_nodes']\n",
        "                else:\n",
        "                    num_nodes_score = 0\n",
        "                sim_score_score = (max_val_val['sim_score'] / sim_score_max) * max_weights['sim_score']\n",
        "                kg_deg_score = (max_val_val['kg_deg'] / kg_deg_max) * max_weights['kg_deg']\n",
        "                max_similarity_scores[key][max_val_key]['agg_score'] = round((\n",
        "                    num_nodes_score + sim_score_score + kg_deg_score), 3)\n",
        "        return max_similarity_scores\n",
        "\n",
        "    def filter_max_simiarity_scores(self, weights=None, max_weights=None):\n",
        "        max_similarity_scores = self.calculate_scores(weights, max_weights)\n",
        "        filtered_max_similarity_scores = {}\n",
        "        for val in max_similarity_scores.values():\n",
        "            for key, values in val.items():\n",
        "                if key not in self.remove_words:\n",
        "                    filtered_max_similarity_scores[key] = values['agg_score']\n",
        "        return sorted(filtered_max_similarity_scores.items(), key=lambda x: x[1], reverse=True)[:5]\n",
        "\n",
        "    def get_movie(self, weights=None, max_weights=None):\n",
        "        filtered_list = self.filter_max_simiarity_scores(weights, max_weights)\n",
        "        movies = {}\n",
        "        filtered_list_terms = [i[0] for i in filtered_list]\n",
        "        for term in filtered_list_terms:\n",
        "            for bfs_term, values in bfsgraph.queue.items():\n",
        "                if term in values:\n",
        "                    movies[term] = bfsgraph.class_movie_rev_map[bfs_term]    \n",
        "        return movies\n",
        "    \n",
        "    def get_class(self, weights=None, max_weights=None):\n",
        "        movies = self.get_movie(weights, max_weights)\n",
        "        top_terms = get_top_tfidf_terms(df_tfidf(process_text('class_corpus.csv', \n",
        "                                                'tfidf')), self.num_terms, value_range=True)\n",
        "        movie_map = defaultdict(dict)\n",
        "        for key, values in movies.items():\n",
        "            for movie in values:\n",
        "                movie_top_terms = top_terms[movie]\n",
        "                sim_score = 0\n",
        "                for top_term in movie_top_terms:\n",
        "                    if top_term in self.word_vectors_corpus.wv.vocab:\n",
        "                        sim_score += self.word_vectors_corpus.similarity(key, top_term)\n",
        "                    elif top_term in self.word_vectors.wv.vocab:\n",
        "                        sim_score += self.word_vectors.similarity(key, top_term)\n",
        "                    else:\n",
        "                        pass\n",
        "                movie_map[movie][key] = sim_score\n",
        "\n",
        "        movie_map_class = {}\n",
        "        for key, values in movie_map.items():\n",
        "            val_list = list(values.values())\n",
        "            key_list = list(values.keys())\n",
        "            index_num = val_list.index(max(val_list))\n",
        "            movie_map_class[key] = key_list[index_num]\n",
        "\n",
        "        return movie_map_class\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ZPWZPN_psiG",
        "outputId": "734c1da9-49b4-48a1-b372-205489a3dfbe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        }
      },
      "source": [
        "num_terms = 13\n",
        "kg = KnowledgeGraph()\n",
        "kg.process_kg('tfidf', num_terms=num_terms)\n",
        "bfsgraph = BFSGraph(kg.graph)\n",
        "bfsgraph.run()\n",
        "wv_results = WordVectors(kg, bfsgraph, word_vectors, word_vectors_corpus, remove_words, num_terms)\n",
        "sim_scores = wv_results.get_class(weights = [0.2, 0.1, 0.7], max_weights={'num_nodes': 0.4, 'sim_score': 0.3, 'kg_deg': 0.3})\n",
        "set(sim_scores.values())\n",
        "# print(len(sim_scores))\n",
        "# set(sim_scores.values())\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:14: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:30: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:31: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:125: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:126: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'characters', 'effects', 'predator', 'replicants', 'terminator'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8CUf8upKfiR0"
      },
      "source": [
        "class PrepareDataNN:\n",
        "    def __init__(self, classes, method, split_sent=False):\n",
        "        self.classes = classes\n",
        "        self.text = process_text('class_corpus.csv', method)\n",
        "        self.split_sent = split_sent\n",
        "\n",
        "    def join_data(self):\n",
        "        text_dict = {self.text[0][i]: self.text[1][i] for i in range(len(self.text[0]))}        \n",
        "        x_train = []\n",
        "        y_train = []\n",
        "        if self.split_sent == False:    \n",
        "            for key in self.classes.keys():\n",
        "                x_train.append(text_dict.get(key))\n",
        "                y_train.append(self.classes.get(key))\n",
        "        elif self.split_sent == True:\n",
        "            for key in self.classes.keys():\n",
        "                temp_text = text_dict.get(key)\n",
        "                num_seq = 1\n",
        "                seq_len = len(temp_text) // num_seq\n",
        "                for i in range(num_seq):\n",
        "                    x_train.append(temp_text[i*seq_len: (i+1)*seq_len])\n",
        "                    y_train.append(self.classes.get(key))\n",
        "        return x_train, y_train\n",
        "\n",
        "    def prepare_data(self):\n",
        "        x_train, y_train = self.join_data()\n",
        "        self.tokenizer = Tokenizer()\n",
        "        self.tokenizer.fit_on_texts(x_train)\n",
        "        sequences = self.tokenizer.texts_to_sequences(x_train)\n",
        "        y_train = LabelEncoder().fit_transform(list(y_train))\n",
        "        sequence_lengths = [np.shape(i)[0] for i in sequences]\n",
        "        max_len = int(max(sequence_lengths) * 0.8)\n",
        "        x_train = preprocessing.sequence.pad_sequences(sequences, maxlen = max_len)\n",
        "        return x_train, y_train\n",
        "\n",
        "    def split_data(self):\n",
        "        train_data, train_labels = self.prepare_data()\n",
        "        train_data, val_data, train_labels, val_labels = train_test_split(\n",
        "            train_data, train_labels, test_size = 0.2)\n",
        "        return train_data, val_data, train_labels, val_labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PzIUNCzBltqT"
      },
      "source": [
        "class BuildModel:\n",
        "    def __init__(self, sim_scores, method, split_sent=True):\n",
        "        self.prepare_data = PrepareDataNN(sim_scores, method, split_sent=split_sent)\n",
        "        self.x_train, self.x_val, self.y_train, self.y_val = self.prepare_data.split_data()\n",
        "        keras.backend.clear_session()\n",
        "\n",
        "    def model_structure(self):\n",
        "        vocab_size = len(self.prepare_data.tokenizer.word_index) + 1\n",
        "        output_size = len(set(self.y_train))\n",
        "        self.model = Sequential()\n",
        "        self.model.add(Embedding(vocab_size, 50, input_length=len(self.x_train[0])))\n",
        "        self.model.add(LSTM(20))\n",
        "        self.model.add(Dropout(0.5))\n",
        "        self.model.add(Dense(30, activation='relu'))\n",
        "        self.model.add(Dense(output_size, activation='softmax'))\n",
        "\n",
        "    def model_compile(self):\n",
        "        self.model.compile(\n",
        "            loss = 'sparse_categorical_crossentropy',\n",
        "            optimizer = 'adam',\n",
        "            metrics = ['accuracy'])\n",
        "\n",
        "    def model_fit(self):\n",
        "        self.model.fit(self.x_train, \n",
        "                       self.y_train, \n",
        "                       epochs = 40, \n",
        "                       validation_data = (self.x_val, self.y_val))\n",
        "\n",
        "    def run(self):\n",
        "        self.model_structure()\n",
        "        self.model_compile()\n",
        "        print(self.model.summary())\n",
        "        self.model_fit()\n",
        "        return self.model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uqtcwYp5o7r6",
        "outputId": "2bd23994-4969-4d29-a3be-cc17d2a503da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model = BuildModel(sim_scores, 'doc2vec').run()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 173, 50)           216700    \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 20)                5680      \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 20)                0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 30)                630       \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 5)                 155       \n",
            "=================================================================\n",
            "Total params: 223,165\n",
            "Trainable params: 223,165\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/40\n",
            "2/2 [==============================] - 1s 257ms/step - loss: 1.6081 - accuracy: 0.2683 - val_loss: 1.6059 - val_accuracy: 0.3636\n",
            "Epoch 2/40\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 1.6018 - accuracy: 0.3902 - val_loss: 1.6022 - val_accuracy: 0.4545\n",
            "Epoch 3/40\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 1.5940 - accuracy: 0.5366 - val_loss: 1.5987 - val_accuracy: 0.4545\n",
            "Epoch 4/40\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 1.5915 - accuracy: 0.5122 - val_loss: 1.5948 - val_accuracy: 0.4545\n",
            "Epoch 5/40\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 1.5825 - accuracy: 0.6341 - val_loss: 1.5903 - val_accuracy: 0.4545\n",
            "Epoch 6/40\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 1.5733 - accuracy: 0.6341 - val_loss: 1.5856 - val_accuracy: 0.4545\n",
            "Epoch 7/40\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 1.5694 - accuracy: 0.5854 - val_loss: 1.5813 - val_accuracy: 0.4545\n",
            "Epoch 8/40\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 1.5596 - accuracy: 0.6341 - val_loss: 1.5773 - val_accuracy: 0.4545\n",
            "Epoch 9/40\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 1.5503 - accuracy: 0.5610 - val_loss: 1.5727 - val_accuracy: 0.4545\n",
            "Epoch 10/40\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 1.5340 - accuracy: 0.6829 - val_loss: 1.5677 - val_accuracy: 0.4545\n",
            "Epoch 11/40\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 1.5271 - accuracy: 0.6585 - val_loss: 1.5623 - val_accuracy: 0.4545\n",
            "Epoch 12/40\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 1.5038 - accuracy: 0.7561 - val_loss: 1.5558 - val_accuracy: 0.4545\n",
            "Epoch 13/40\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 1.4906 - accuracy: 0.7073 - val_loss: 1.5468 - val_accuracy: 0.4545\n",
            "Epoch 14/40\n",
            "2/2 [==============================] - 0s 45ms/step - loss: 1.4680 - accuracy: 0.7561 - val_loss: 1.5353 - val_accuracy: 0.4545\n",
            "Epoch 15/40\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 1.4244 - accuracy: 0.7561 - val_loss: 1.5195 - val_accuracy: 0.4545\n",
            "Epoch 16/40\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 1.4001 - accuracy: 0.6829 - val_loss: 1.4994 - val_accuracy: 0.4545\n",
            "Epoch 17/40\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 1.3661 - accuracy: 0.7073 - val_loss: 1.4759 - val_accuracy: 0.5455\n",
            "Epoch 18/40\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 1.3087 - accuracy: 0.6829 - val_loss: 1.4421 - val_accuracy: 0.5455\n",
            "Epoch 19/40\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 1.2364 - accuracy: 0.6341 - val_loss: 1.3916 - val_accuracy: 0.5455\n",
            "Epoch 20/40\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 1.1834 - accuracy: 0.6829 - val_loss: 1.3181 - val_accuracy: 0.5455\n",
            "Epoch 21/40\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 1.1435 - accuracy: 0.6098 - val_loss: 1.2255 - val_accuracy: 0.5455\n",
            "Epoch 22/40\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 1.0368 - accuracy: 0.6341 - val_loss: 1.2140 - val_accuracy: 0.5455\n",
            "Epoch 23/40\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.9979 - accuracy: 0.6341 - val_loss: 1.2825 - val_accuracy: 0.4545\n",
            "Epoch 24/40\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.9322 - accuracy: 0.7561 - val_loss: 1.3039 - val_accuracy: 0.4545\n",
            "Epoch 25/40\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.9392 - accuracy: 0.8049 - val_loss: 1.2806 - val_accuracy: 0.4545\n",
            "Epoch 26/40\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.9182 - accuracy: 0.6829 - val_loss: 1.1851 - val_accuracy: 0.5455\n",
            "Epoch 27/40\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.8570 - accuracy: 0.6829 - val_loss: 1.0717 - val_accuracy: 0.5455\n",
            "Epoch 28/40\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.8560 - accuracy: 0.6585 - val_loss: 1.5679 - val_accuracy: 0.4545\n",
            "Epoch 29/40\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.8850 - accuracy: 0.6341 - val_loss: 1.2240 - val_accuracy: 0.4545\n",
            "Epoch 30/40\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.7836 - accuracy: 0.6585 - val_loss: 1.1336 - val_accuracy: 0.5455\n",
            "Epoch 31/40\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.7364 - accuracy: 0.7073 - val_loss: 1.2942 - val_accuracy: 0.4545\n",
            "Epoch 32/40\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.7059 - accuracy: 0.7073 - val_loss: 1.3588 - val_accuracy: 0.3636\n",
            "Epoch 33/40\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.7200 - accuracy: 0.7073 - val_loss: 1.4020 - val_accuracy: 0.3636\n",
            "Epoch 34/40\n",
            "2/2 [==============================] - 0s 42ms/step - loss: 0.7079 - accuracy: 0.7317 - val_loss: 1.4243 - val_accuracy: 0.3636\n",
            "Epoch 35/40\n",
            "2/2 [==============================] - 0s 43ms/step - loss: 0.6439 - accuracy: 0.6829 - val_loss: 1.4288 - val_accuracy: 0.3636\n",
            "Epoch 36/40\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.6729 - accuracy: 0.7073 - val_loss: 1.4333 - val_accuracy: 0.3636\n",
            "Epoch 37/40\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.6161 - accuracy: 0.7561 - val_loss: 1.4486 - val_accuracy: 0.3636\n",
            "Epoch 38/40\n",
            "2/2 [==============================] - 0s 44ms/step - loss: 0.6237 - accuracy: 0.7073 - val_loss: 1.4614 - val_accuracy: 0.3636\n",
            "Epoch 39/40\n",
            "2/2 [==============================] - 0s 42ms/step - loss: 0.6046 - accuracy: 0.6829 - val_loss: 1.4704 - val_accuracy: 0.3636\n",
            "Epoch 40/40\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.6019 - accuracy: 0.6829 - val_loss: 1.4840 - val_accuracy: 0.3636\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}